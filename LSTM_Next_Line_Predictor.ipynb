{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjTqPc8JTfzDNTaTvXsTLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g-e-mm/LSTM_Predictor/blob/main/LSTM_Next_Line_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approach to the problem\n",
        "\n",
        "1. Load the libraries and data\n",
        "2. Clean the data - remove commas, periods etc.\n",
        "3. Tokenize the data\n",
        "4. Convert to sequence\n",
        "5. Input sequence and Output sequence\n",
        "6. Create sequential model\n",
        "7. LSTM Layers\n",
        "8. Compile and fit the model\n",
        "9. Evaluate using inputs"
      ],
      "metadata": {
        "id": "kMSY5vmJdPNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the libraries and data"
      ],
      "metadata": {
        "id": "zDYWKsAEd-sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt?_sm_au_=iVV10f0f2kPt2J07\n"
      ],
      "metadata": {
        "id": "-xxZ_cXvdLoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding, Dropout\n",
        "from keras.layers import Bidirectional, Dropout\n",
        "import string\n",
        "import urllib"
      ],
      "metadata": {
        "id": "dEToD38mdKtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data using urllib"
      ],
      "metadata": {
        "id": "LBjozuPqkN9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3DtGjvyckjQ"
      },
      "outputs": [],
      "source": [
        "response= urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt?_sm_au_=iVV10f0f2kPt2J07')\n",
        "doc= response.read().decode('utf8')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc[:200])"
      ],
      "metadata": {
        "id": "t55sonMQkkyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Data"
      ],
      "metadata": {
        "id": "xuK_Tp8JlJrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_doc(doc):\n",
        "    #replace '--' with a space ' '\n",
        "    doc= doc.replace('--', ' ')\n",
        "    #split into tokens by white space\n",
        "    tokens= doc.split()\n",
        "    #remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens= [w.translate(table) for w in tokens]\n",
        "    #remove remaining tokens that are not alphabetic\n",
        "    tokens= [word for word in tokens if word.isalpha()]\n",
        "    #make lower case\n",
        "    tokens= [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "vJ0zGjsXlw0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = clean_doc(doc)"
      ],
      "metadata": {
        "id": "iK9qwCtrmaBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the first five tokens are: \",tokens[:5])\n",
        "print(\"Total no. of tokens: \",len(tokens))\n",
        "print(\"Total no. of unique tokens: \",len(set(tokens)))"
      ],
      "metadata": {
        "id": "HtxM4qxXmgf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'book' in tokens:\n",
        "    count= tokens.count('book')\n",
        "    print(count)"
      ],
      "metadata": {
        "id": "dDB6FVqyn2NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing and Sequencing the data"
      ],
      "metadata": {
        "id": "htKUPgMulweS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length =50+1\n",
        "sequence = list()\n",
        "for i in range(length,len(tokens)):\n",
        "    #select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    #convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    #store\n",
        "    sequence.append(line)\n",
        "print(len(sequence))\n",
        "print(sequence[:10])"
      ],
      "metadata": {
        "id": "6oJK1kkOoj0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving the sequences in a file as a backup. Also the same is being loaded"
      ],
      "metadata": {
        "id": "UowGSn5drDrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving tokens to a file - one dialog per line\n",
        "def save_doc(lines,filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename,'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "#save sequences to file\n",
        "out_filename = \"rep_sequences.txt\"\n",
        "save_doc(sequence,out_filename)"
      ],
      "metadata": {
        "id": "Q43dpjBXpbQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "    file = open(filename,'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "#load\n",
        "in_filename = 'rep_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "metadata": {
        "id": "ADshhTskrNsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenize and convert to sequences"
      ],
      "metadata": {
        "id": "fLAtqDadsBz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequence = tokenizer.texts_to_sequences(lines)\n",
        "sequence = pad_sequences(sequence)\n",
        "print"
      ],
      "metadata": {
        "id": "II7r3n1BsKv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rFf08-uQsSb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = array(sequence)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ITIaIA1Ysl-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "id": "Me2rX55fswBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "declaring x and y"
      ],
      "metadata": {
        "id": "a3dNUyaYtQgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = sequences[:,:-1],sequences[:,-1]\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "6tJhqtFztOif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the sequential model"
      ],
      "metadata": {
        "id": "AbydaiqIvzQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "bPRrmvJHv7Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "_ONSkY-twA21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,10,input_length=50))\n",
        "\n",
        "# Adding a Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding another LSTM layer for capturing more complex patterns\n",
        "model.add(LSTM(100, return_sequences=True)) # Added return_sequences=True\n",
        "\n",
        "# Adding another Dense layer with ReLU activation for non-linearity\n",
        "model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# Adding another Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer for capturing even more complex patterns\n",
        "model.add(LSTM(100))\n",
        "\n",
        "# Adding a third Dense layer with ReLU activation for non-linearity\n",
        "model.add(Dense(100, activation='relu'))\n",
        "\n",
        "# Adding a third Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer for predicting the next word\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "print(model.summary())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OABcqDFr5c_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "34l3-wJVyTNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(x,y,batch_size=256,epochs=100)"
      ],
      "metadata": {
        "id": "RF3HAjJyyXFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pre-pad sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=max_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict(encoded, verbose=0)\n",
        "\t\t# get index of the most probable word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat.argmax():\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\treturn in_text\n",
        "\n",
        "# evaluate model\n",
        "print(generate_seq(model, tokenizer, 50, 'I went down yesterday to the Piraeus with Glaucon the son of Ariston,', 10))\n"
      ],
      "metadata": {
        "id": "JuHGfPRSCJla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the expected output:\n",
        "\n",
        "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
        "\n",
        "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
        "\n",
        "Artemis.); <br> as such the"
      ],
      "metadata": {
        "id": "bpmmpX_yCsIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "iifzZ8qug2Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_text(seed_text):\n",
        "  generated_text = generate_seq(model, tokenizer, 50, seed_text, 10)\n",
        "  return generated_text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter seed text here...\"), # Changed from gr.inputs.Textbox to gr.Textbox\n",
        "    outputs=\"text\",\n",
        "    title=\"Text Generation with LSTM\",\n",
        "    description=\"Generate text based on a seed text using a trained LSTM model.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yebFoMi-JdeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}